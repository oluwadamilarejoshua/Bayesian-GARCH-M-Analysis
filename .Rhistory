cat("95% Credible Intervals:\n", credible_intervals, "\n")
# Plot trace plots for each parameter
par(mfrow=c(2, 2))  # Arrange the plots in a 2x2 grid
plot(mcmc_samples[, 1], main="Trace Plot: Parameter 1")
plot(mcmc_samples[, 2], main="Trace Plot: Parameter 2")
plot(mcmc_samples[, 3], main="Trace Plot: Parameter 3")
plot(mcmc_samples[, 4], main="Trace Plot: Parameter 4")
# Run diagnostics
for (i in 1:4) {
print(paste("Parameter", i))
print(geweke.diag(mcmc_samples[, i]))  # Geweke Diagnostic
print(effectiveSize(mcmc_samples[, i]))  # Effective Sample Size
}
n_iter <- 200000
proposal_sd <- 0.06
samples <- metropolis(init, y, X, n_iter, proposal_sd)
# Posterior means and credible intervals
posterior_means <- colMeans(samples)
credible_intervals <- apply(samples, 2, quantile, probs = c(0.025, 0.975))
# Print results
cat("Posterior Means:\n", posterior_means, "\n")
cat("95% Credible Intervals:\n", credible_intervals, "\n")
library(coda)
# Convert samples matrix to mcmc object (for analysis with coda)
mcmc_samples <- as.mcmc(samples)
# Plot trace plots for each parameter
par(mfrow=c(2, 2))  # Arrange the plots in a 2x2 grid
plot(mcmc_samples[, 1], main="Trace Plot: Parameter 1")
plot(mcmc_samples[, 2], main="Trace Plot: Parameter 2")
plot(mcmc_samples[, 3], main="Trace Plot: Parameter 3")
plot(mcmc_samples[, 4], main="Trace Plot: Parameter 4")
# Plot posterior distributions
par(mfrow = c(3, 2))
for (i in 1:ncol(samples)) {
hist(samples[, i], main = paste("Parameter", i), xlab = "Value",
probability = TRUE)
}
# Convert samples matrix to mcmc object (for analysis with coda)
mcmc_samples <- as.mcmc(samples)
# Plot trace plots for each parameter
par(mfrow=c(2, 2))  # Arrange the plots in a 2x2 grid
plot(mcmc_samples[, 1], main="Trace Plot: Parameter 1")
plot(mcmc_samples[, 2], main="Trace Plot: Parameter 2")
plot(mcmc_samples[, 3], main="Trace Plot: Parameter 3")
plot(mcmc_samples[, 4], main="Trace Plot: Parameter 4")
# Run diagnostics
for (i in 1:4) {
print(paste("Parameter", i))
print(geweke.diag(mcmc_samples[, i]))  # Geweke Diagnostic
print(effectiveSize(mcmc_samples[, i]))  # Effective Sample Size
}
source('Unit Root Evaluation and Handling.R')
# Preliminary model (OLS estimation) for informative prior ----------------
# Preliminary OLS model
ols_model <- lm(y ~ X)
summary(ols_model)
# Extract coefficients and residual variance
ols_coefficients <- coef(ols_model)
ols_variance <- var(residuals(ols_model))
# Priors for beta and mu
prior_mu <- ols_coefficients[1]  # Intercept
prior_beta <- ols_coefficients[-1]  # Coefficients for predictors
prior_variance <- ols_variance  # Prior variance for residuals
# Prior, Likelihood, and Posterior functions ------------------------------
# Defining Prior
log_prior <- function(params) {
mu <- params[1]
gamma <- params[2]
beta_X <- params[3:(2 + ncol(X))]
omega <- params[3 + ncol(X)]
alpha <- params[4 + ncol(X)]
beta <- params[5 + ncol(X)]
# Priors based on empirical evidence
prior_mu <- dnorm(mu, prior_mu, sqrt(prior_variance), log = TRUE)
prior_gamma <- dnorm(gamma, 0, 10, log = TRUE)
prior_beta_X <- sum(dnorm(beta_X, prior_beta, sqrt(prior_variance), log = TRUE))
prior_omega <- ifelse(omega > 0 & omega < 1, 0, -Inf)
prior_alpha <- ifelse(alpha > 0 & alpha < 1, 0, -Inf)
prior_beta <- ifelse(beta > 0 & beta < 1, 0, -Inf)
return(prior_mu + prior_gamma + prior_beta_X + prior_omega + prior_alpha + prior_beta)
}
# Defining log-likelihood function
log_likelihood <- function(params, y, X) {
mu <- params[1]
gamma <- params[2]
beta_X <- params[3:(2 + ncol(X))]
omega <- params[3 + ncol(X)]
alpha <- params[4 + ncol(X)]
beta <- params[5 + ncol(X)]
n <- length(y)
h <- numeric(n)
h[1] <- omega / (1 - alpha - beta)
log_lik <- 0
for (t in 2:n) {
h[t] <- omega + alpha *
(y[t-1] - mu - gamma * h[t-1] - sum(X[t-1, ] * beta_X))^2 + beta * h[t-1]
log_lik <- log_lik - 0.5 *
(log(2 * pi) + log(h[t]) + (y[t] - mu - gamma *
h[t] - sum(X[t, ] * beta_X))^2 / h[t])
}
return(log_lik)
}
# Combining Prior and Likelihood for posterior
log_posterior <- function(params, y, X) {
# Extract parameters
beta_0 <- params[1]
beta_X <- params[2:(1 + ncol(X))]
omega <- params[2 + ncol(X)]
alpha <- params[3 + ncol(X)]
beta <- params[4 + ncol(X)]
# Compute the log-prior
log_prior <- 0
if (omega <= 0 || alpha <= 0 || beta <= 0 || (alpha + beta) >= 1) {
return(-Inf)  # Invalid GARCH parameters
}
log_prior <- sum(dnorm(beta_X, mean = 0, sd = 1, log = TRUE)) +
dgamma(omega, shape = 2, rate = 1, log = TRUE) +
dbeta(alpha, 2, 2, log = TRUE) +
dbeta(beta, 2, 2, log = TRUE)
# Compute the log-likelihood
n <- length(y)
h <- numeric(n)
h[1] <- var(y)  # Initialize variance
log_likelihood <- 0
for (t in 2:n) {
h[t] <- omega + alpha * (y[t - 1]^2) + beta * h[t - 1]
if (h[t] <= 0) {
return(-Inf)  # Invalid variance
}
mu_t <- beta_0 + sum(beta_X * X[t, ])
log_likelihood <- log_likelihood + dnorm(y[t], mean = mu_t, sd = sqrt(h[t]), log = TRUE)
}
# Total log-posterior
log_posterior_value <- log_prior + log_likelihood
# Debugging output
cat("Log Prior:", log_prior, "\n")
cat("Log Likelihood:", log_likelihood, "\n")
cat("Log Posterior:", log_posterior_value, "\n")
return(log_posterior_value)
}
# source('Prior, Likelihood, and Posterior functions.R')
# Define the Metropolis algorithm
metropolis <- function(init, y, X, n_iter, proposal_sd) {
params <- init
samples <- matrix(NA, nrow = n_iter, ncol = length(init))
accept <- 0
for (i in 1:n_iter) {
proposal <- rnorm(length(init), mean = params, sd = proposal_sd)
log_accept_ratio <- log_posterior(proposal, y, X) - log_posterior(params, y, X)
# Debugging and safeguard
if (is.na(log_accept_ratio) || is.nan(log_accept_ratio)) {
cat("Invalid log_accept_ratio at iteration", i, "\n")
cat("Proposed Parameters:", proposal, "\n")
cat("Log Posterior (proposal):", log_posterior(proposal, y, X), "\n")
cat("Log Posterior (current):", log_posterior(params, y, X), "\n")
stop("log_accept_ratio is invalid. Check your log_posterior function.")
}
if (log(runif(1)) < log_accept_ratio) {
params <- proposal
accept <- accept + 1
}
samples[i, ] <- params
}
cat("Acceptance rate:", accept / n_iter, "\n")
return(samples)
}
# Initial values for parameters
# init <- c(prior_mu, 0.1, rep(0.1, ncol(X)), 0.1, 0.1, 0.1)
ols_coefs <- coef(ols_model)
ols_sigma <- summary(ols_model)$sigma^2
init <- c(ols_coefs[1],   # Intercept
ols_sigma,      # Residual variance
ols_coefs[-1],  # Slopes for predictors
rep(ols_sigma, 3))
# Run MCMC
n_iter <- 200000
proposal_sd <- 0.06
samples <- metropolis(init, y, X, n_iter, proposal_sd)
library(readxl)
library(tidyverse)
library(GGally)
data <- read_excel("../Data/ResearchData.xlsx")
# Scaling the monetary variables by dividing by 1 million
data_scaled <- data
data_scaled[, c("Debt service on external debt, total (TDS, current US$)",
"GDP (current US$)")] <-
data[, c("Debt service on external debt, total (TDS, current US$)",
"GDP (current US$)")] / 1e6
# Log transformation
data_transformed <- data_scaled
data_transformed[, c("Debt service on external debt, total (TDS, current US$)",
"GDP (current US$)",
"Real effective exchange rate index (2010 = 100)")] <-
log(data_scaled[, c("Debt service on external debt, total (TDS, current US$)",
"GDP (current US$)",
"Real effective exchange rate index (2010 = 100)")])
data_transformed <- as.data.frame(data_transformed)
colnames(data_transformed) <- c('Year','Debt', 'GDP', 'ExchangeRate')
head(data_transformed)
# Summary of the data
summary(data_transformed)
# Create a subset for plotting
data_subset <- data_transformed[, c("ExchangeRate", "Debt", "GDP")]
# Create pair Scatter plot using GGally
ggpairs(data_subset,
title = "Scatter Plots of Exchange Rate and Predictors",
progress = FALSE)
source("Data Preparation.R")
# Load necessary library
library(tseries)
# Extracting columns
ExchangeRate <- data_subset$ExchangeRate
GDP <- data_subset$GDP
Debt <- data_subset$Debt
# Function to check stationarity for a single variable
check_stationarity <- function(series, series_name) {
cat("\n--- Stationarity Check for:", series_name, "---\n")
# ADF Test
adf_result <- adf.test(series, alternative = "stationary")
cat("ADF Test:\n")
cat("  p-value:", adf_result$p.value, "\n")
if (adf_result$p.value < 0.05) {
cat("  The series is stationary (reject null of unit root).\n")
} else {
cat("  The series is not stationary (fail to reject null of unit root).\n")
}
# KPSS Test
kpss_result <- kpss.test(series, null = "Level")
cat("\nKPSS Test:\n")
cat("  p-value:", kpss_result$p.value, "\n")
if (kpss_result$p.value > 0.05) {
cat("  The series is stationary (fail to reject null of stationarity).\n")
} else {
cat("  The series is not stationary (reject null of stationarity).\n")
}
}
# Apply stationarity tests to ExchangeRate, GDP, and Debt
check_stationarity(ExchangeRate, "Exchange Rate")
check_stationarity(GDP, "GDP")
check_stationarity(Debt, "Debt")
# Applying first-order differencing
# ExchangeRate_diff <- diff(ExchangeRate)
ExchangeRate_diff <- ExchangeRate[-1]
GDP_diff <- diff(GDP)
# Debt_diff <- diff(Debt)
Debt_diff <- Debt[-1]
# check_stationarity(ExchangeRate_diff, "Differenced Exchange Rate")
# check_stationarity(GDP_diff, "Differenced GDP")
# check_stationarity(Debt_diff, "Differenced Debt")
# Assign variables
y <- ExchangeRate_diff
X <- as.matrix(cbind(Debt_diff, GDP_diff))
source('Unit Root Evaluation and Handling.R')
# Preliminary model (OLS estimation) for informative prior ----------------
# Preliminary OLS model
ols_model <- lm(y ~ X)
summary(ols_model)
# Extract coefficients and residual variance
ols_coefficients <- coef(ols_model)
ols_variance <- var(residuals(ols_model))
# Priors for beta and mu
prior_mu <- ols_coefficients[1]  # Intercept
prior_beta <- ols_coefficients[-1]  # Coefficients for predictors
prior_variance <- ols_variance  # Prior variance for residuals
# Prior, Likelihood, and Posterior functions ------------------------------
# Defining Prior
log_prior <- function(params) {
mu <- params[1]
gamma <- params[2]
beta_X <- params[3:(2 + ncol(X))]
omega <- params[3 + ncol(X)]
alpha <- params[4 + ncol(X)]
beta <- params[5 + ncol(X)]
# Priors based on empirical evidence
prior_mu <- dnorm(mu, prior_mu, sqrt(prior_variance), log = TRUE)
prior_gamma <- dnorm(gamma, 0, 10, log = TRUE)
prior_beta_X <- sum(dnorm(beta_X, prior_beta, sqrt(prior_variance), log = TRUE))
prior_omega <- ifelse(omega > 0 & omega < 1, 0, -Inf)
prior_alpha <- ifelse(alpha > 0 & alpha < 1, 0, -Inf)
prior_beta <- ifelse(beta > 0 & beta < 1, 0, -Inf)
return(prior_mu + prior_gamma + prior_beta_X + prior_omega + prior_alpha + prior_beta)
}
# Defining log-likelihood function
log_likelihood <- function(params, y, X) {
mu <- params[1]
gamma <- params[2]
beta_X <- params[3:(2 + ncol(X))]
omega <- params[3 + ncol(X)]
alpha <- params[4 + ncol(X)]
beta <- params[5 + ncol(X)]
n <- length(y)
h <- numeric(n)
h[1] <- omega / (1 - alpha - beta)
log_lik <- 0
for (t in 2:n) {
h[t] <- omega + alpha *
(y[t-1] - mu - gamma * h[t-1] - sum(X[t-1, ] * beta_X))^2 + beta * h[t-1]
log_lik <- log_lik - 0.5 *
(log(2 * pi) + log(h[t]) + (y[t] - mu - gamma *
h[t] - sum(X[t, ] * beta_X))^2 / h[t])
}
return(log_lik)
}
# Combining Prior and Likelihood for posterior
log_posterior <- function(params, y, X) {
# Extract parameters
beta_0 <- params[1]
beta_X <- params[2:(1 + ncol(X))]
omega <- params[2 + ncol(X)]
alpha <- params[3 + ncol(X)]
beta <- params[4 + ncol(X)]
# Compute the log-prior
log_prior <- 0
if (omega <= 0 || alpha <= 0 || beta <= 0 || (alpha + beta) >= 1) {
return(-Inf)  # Invalid GARCH parameters
}
log_prior <- sum(dnorm(beta_X, mean = 0, sd = 1, log = TRUE)) +
dgamma(omega, shape = 2, rate = 1, log = TRUE) +
dbeta(alpha, 2, 2, log = TRUE) +
dbeta(beta, 2, 2, log = TRUE)
# Compute the log-likelihood
n <- length(y)
h <- numeric(n)
h[1] <- var(y)  # Initialize variance
log_likelihood <- 0
for (t in 2:n) {
h[t] <- omega + alpha * (y[t - 1]^2) + beta * h[t - 1]
if (h[t] <= 0) {
return(-Inf)  # Invalid variance
}
mu_t <- beta_0 + sum(beta_X * X[t, ])
log_likelihood <- log_likelihood + dnorm(y[t], mean = mu_t, sd = sqrt(h[t]), log = TRUE)
}
# Total log-posterior
log_posterior_value <- log_prior + log_likelihood
# Debugging output
cat("Log Prior:", log_prior, "\n")
cat("Log Likelihood:", log_likelihood, "\n")
cat("Log Posterior:", log_posterior_value, "\n")
return(log_posterior_value)
}
# source('Prior, Likelihood, and Posterior functions.R')
# Define the Metropolis algorithm
metropolis <- function(init, y, X, n_iter, proposal_sd) {
params <- init
samples <- matrix(NA, nrow = n_iter, ncol = length(init))
accept <- 0
for (i in 1:n_iter) {
proposal <- rnorm(length(init), mean = params, sd = proposal_sd)
log_accept_ratio <- log_posterior(proposal, y, X) - log_posterior(params, y, X)
# Debugging and safeguard
if (is.na(log_accept_ratio) || is.nan(log_accept_ratio)) {
cat("Invalid log_accept_ratio at iteration", i, "\n")
cat("Proposed Parameters:", proposal, "\n")
cat("Log Posterior (proposal):", log_posterior(proposal, y, X), "\n")
cat("Log Posterior (current):", log_posterior(params, y, X), "\n")
stop("log_accept_ratio is invalid. Check your log_posterior function.")
}
if (log(runif(1)) < log_accept_ratio) {
params <- proposal
accept <- accept + 1
}
samples[i, ] <- params
}
cat("Acceptance rate:", accept / n_iter, "\n")
return(samples)
}
# Initial values for parameters
# init <- c(prior_mu, 0.1, rep(0.1, ncol(X)), 0.1, 0.1, 0.1)
ols_coefs <- coef(ols_model)
ols_sigma <- summary(ols_model)$sigma^2
init <- c(ols_coefs[1],   # Intercept
ols_sigma,      # Residual variance
ols_coefs[-1],  # Slopes for predictors
rep(ols_sigma, 3))
# Run MCMC
n_iter <- 200000
proposal_sd <- 0.06
samples <- metropolis(init, y, X, n_iter, proposal_sd)
proposal_sd <- 0.005
samples <- metropolis(init, y, X, n_iter, proposal_sd)
proposal_sd <- 0.5
samples <- metropolis(init, y, X, n_iter, proposal_sd)
View(log_posterior)
source('Unit Root Evaluation and Handling.R')
# Preliminary model (OLS estimation) for informative prior ----------------
# Preliminary OLS model
ols_model <- lm(y ~ X)
summary(ols_model)
# Extract coefficients and residual variance
ols_coefficients <- coef(ols_model)
ols_variance <- var(residuals(ols_model))
# Priors for beta and mu
prior_mu <- ols_coefficients[1]  # Intercept
prior_beta <- ols_coefficients[-1]  # Coefficients for predictors
prior_variance <- ols_variance  # Prior variance for residuals
# Prior, Likelihood, and Posterior functions ------------------------------
# Defining Prior
log_prior <- function(params) {
mu <- params[1]
gamma <- params[2]
beta_X <- params[3:(2 + ncol(X))]
omega <- params[3 + ncol(X)]
alpha <- params[4 + ncol(X)]
beta <- params[5 + ncol(X)]
# Priors based on empirical evidence
prior_mu <- dnorm(mu, prior_mu, sqrt(prior_variance), log = TRUE)
prior_gamma <- dnorm(gamma, 0, 10, log = TRUE)
prior_beta_X <- sum(dnorm(beta_X, prior_beta, sqrt(prior_variance), log = TRUE))
prior_omega <- ifelse(omega > 0 & omega < 1, 0, -Inf)
prior_alpha <- ifelse(alpha > 0 & alpha < 1, 0, -Inf)
prior_beta <- ifelse(beta > 0 & beta < 1, 0, -Inf)
return(prior_mu + prior_gamma + prior_beta_X + prior_omega + prior_alpha + prior_beta)
}
# Defining log-likelihood function
log_likelihood <- function(params, y, X) {
mu <- params[1]
gamma <- params[2]
beta_X <- params[3:(2 + ncol(X))]
omega <- params[3 + ncol(X)]
alpha <- params[4 + ncol(X)]
beta <- params[5 + ncol(X)]
n <- length(y)
h <- numeric(n)
h[1] <- omega / (1 - alpha - beta)
log_lik <- 0
for (t in 2:n) {
h[t] <- omega + alpha *
(y[t-1] - mu - gamma * h[t-1] - sum(X[t-1, ] * beta_X))^2 + beta * h[t-1]
log_lik <- log_lik - 0.5 *
(log(2 * pi) + log(h[t]) + (y[t] - mu - gamma *
h[t] - sum(X[t, ] * beta_X))^2 / h[t])
}
return(log_lik)
}
# Combining Prior and Likelihood for posterior
log_posterior <- function(params, y, X) {
# Extract parameters
beta_0 <- params[1]
beta_X <- params[2:(1 + ncol(X))]
omega <- params[2 + ncol(X)]
alpha <- params[3 + ncol(X)]
beta <- params[4 + ncol(X)]
# Compute the log-prior
log_prior <- 0
if (omega <= 0 || alpha <= 0 || beta <= 0 || (alpha + beta) >= 1) {
return(-Inf)  # Invalid GARCH parameters
}
log_prior <- sum(dnorm(beta_X, mean = 0, sd = 1, log = TRUE)) +
dgamma(omega, shape = 2, rate = 1, log = TRUE) +
dbeta(alpha, 2, 2, log = TRUE) +
dbeta(beta, 2, 2, log = TRUE)
# Compute the log-likelihood
n <- length(y)
h <- numeric(n)
h[1] <- var(y)  # Initialize variance
log_likelihood <- 0
for (t in 2:n) {
h[t] <- omega + alpha * (y[t - 1]^2) + beta * h[t - 1]
if (h[t] <= 0) {
return(-Inf)  # Invalid variance
}
mu_t <- beta_0 + sum(beta_X * X[t, ])
log_likelihood <- log_likelihood +
dnorm(y[t], mean = mu_t, sd = sqrt(h[t]), log = TRUE)
}
# Total log-posterior
log_posterior_value <- log_prior + log_likelihood
# Debugging output
cat("Log Prior:", log_prior, "\n")
cat("Log Likelihood:", log_likelihood, "\n")
cat("Log Posterior:", log_posterior_value, "\n")
return(log_posterior_value)
}
# source('Prior, Likelihood, and Posterior functions.R')
# Define the Metropolis algorithm
metropolis <- function(init, y, X, n_iter, proposal_sd) {
params <- init
samples <- matrix(NA, nrow = n_iter, ncol = length(init))
accept <- 0
for (i in 1:n_iter) {
proposal <- rnorm(length(init), mean = params, sd = proposal_sd)
log_accept_ratio <- log_posterior(proposal, y, X) - log_posterior(params, y, X)
# Debugging and safeguard
if (is.na(log_accept_ratio) || is.nan(log_accept_ratio)) {
cat("Invalid log_accept_ratio at iteration", i, "\n")
cat("Proposed Parameters:", proposal, "\n")
cat("Log Posterior (proposal):", log_posterior(proposal, y, X), "\n")
cat("Log Posterior (current):", log_posterior(params, y, X), "\n")
stop("log_accept_ratio is invalid. Check your log_posterior function.")
}
if (log(runif(1)) < log_accept_ratio) {
params <- proposal
accept <- accept + 1
}
samples[i, ] <- params
}
cat("Acceptance rate:", accept / n_iter, "\n")
return(samples)
}
# Initial values for parameters
# init <- c(prior_mu, 0.1, rep(0.1, ncol(X)), 0.1, 0.1, 0.1)
ols_coefs <- coef(ols_model)
ols_sigma <- summary(ols_model)$sigma^2
init <- c(ols_coefs[1],   # Intercept
ols_sigma,      # Residual variance
ols_coefs[-1],  # Slopes for predictors
rep(ols_sigma, 3))
# Run MCMC
n_iter <- 200000
proposal_sd <- 0.5
samples <- metropolis(init, y, X, n_iter, proposal_sd)
